{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Prediction Request to Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the testing and prediction request process is carried out to the model serving using an HTTP request from the model that has been successfully deployed using the cloud platform called Railway. This process sets an endpoint URL that becomes the main API to make requests in the form of an HTTP request by calling the model serving hosted by TF-Serving. The URL is `https://mlops-disaster-tweets-production.up.railway.app/v1/models/tweets-model:predict` and is designed to request predictions from the model.\n",
    "\n",
    "Based on the machine learning pipeline process that was created, it is known that the format of the data received by the model is TFRecord, while the prediction request process in the production environment must be in the form of a JSON string (Javascript Object Notation). Some characters in TFRecord data can be a problem when included directly in JSON because they are considered invalid. Therefore, a coding technique using Base64 encoding is used, which generally represents binary data, so that TFRecord binary data can be safely included in a JSON string without the risk of invalid characters occurring and can be accepted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to run the testing and prediction request to the model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL TF-Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for TF-Serving\n",
    "serving_url = \"https://mlops-disaster-tweets-production.up.railway.app/v1/models/tweets-model:predict\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input String Tweet Text Example and ID Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input string text\n",
    "id_value = 1\n",
    "input_string = \"Just happened a terrible car crash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Encoded the serializede example from TFRecord format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.train.Example from the input string\n",
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'id': tf.train.Feature(int64_list=tf.train.Int64List(value=[id_value])),\n",
    "    'text': tf.train.Feature(bytes_list=tf.train.BytesList(value=[input_string.encode()]))\n",
    "}))\n",
    "tfrecord_example = example.SerializeToString()\n",
    "\n",
    "# base64 encode the serialized example\n",
    "b64_encoded_example = base64.b64encode(tfrecord_example).decode(\"utf-8\")\n",
    "\n",
    "# prepare the input example in the correct format\n",
    "input_example = {\"b64\": b64_encoded_example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Just happened a terrible car crash\n",
      "\n",
      "Model output: 0.914532244\n",
      "Prediction: Disaster tweet\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweet:\", input_string)\n",
    "\n",
    "# send prediction request to TF-Serving\n",
    "response = requests.post(serving_url, json={\"instances\": [input_example]})\n",
    "\n",
    "# check the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    model_output = result[\"predictions\"][0][0]\n",
    "    print(\"\\nModel output:\", model_output)\n",
    "\n",
    "    # check if the model output is spam (assuming threshold is 0.75)\n",
    "    tweet_threshold = 0.75\n",
    "    if model_output > tweet_threshold:\n",
    "        print(\"Prediction: Disaster tweet\")\n",
    "    else:\n",
    "        print(\"Prediction: Not Disaster tweet (just normal tweet)\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output results of the prediction request, it was found that the example tweet used for testing was a Disaster Tweet type tweet with a model output of around 0.91. This result shows that the model works quite well in classifying tweet sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster_tweets01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
