{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengujian dan Prediction Request ke Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini, dilakukan proses pengujian dan prediction request kepada model serving menggunakan HTTP request yang berasal dari model yang sudah berhasil di deploy menggunakan platform cloud bernama Railway. Proses ini menetapkan sebuah URL endpoint yang menjadi API utama untuk melakukan permintaan dalam bentuk HTTP request dengan memanggil model serving yang di-host oleh TF-Serving. URL tersebut adalah `https://mlops-disaster-tweets-production.up.railway.app/v1/models/tweets-model:predict` dan dirancang untuk meminta prediksi dari model.\n",
    "\n",
    "Berdasarkan proses machine learning pipeline yang dibuat, diketahui bahwa format data yang diterima oleh model adalah TFRecord, sedangkan proses permintaan (request) prediksi di production environment harus berupa string JSON (Javascript Object Notation). Beberapa karakter dalam data TFRecord dapat menjadi masalah saat disertakan langsung dalam JSON karena dianggap tidak valid. Oleh karena itu, digunakan teknik pengkodean menggunakan Base64 encoding yang secara umum mewakili data biner, sehingga data biner TFRecord dapat disertakan dengan aman dalam string JSON tanpa resiko terjadinya karakter yang tidak valid dan bisa diterima oleh model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah kode untuk menjalankan pengujian dan prediction request ke model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL TF-Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for TF-Serving\n",
    "serving_url = \"https://mlops-disaster-tweets-production.up.railway.app/v1/models/tweets-model:predict\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input String Tweet Text Example and ID Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input string text\n",
    "id_value = 1\n",
    "input_string = \"Just happened a terrible car crash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Encoded the serializede example from TFRecord format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.train.Example from the input string\n",
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'id': tf.train.Feature(int64_list=tf.train.Int64List(value=[id_value])),\n",
    "    'text': tf.train.Feature(bytes_list=tf.train.BytesList(value=[input_string.encode()]))\n",
    "}))\n",
    "tfrecord_example = example.SerializeToString()\n",
    "\n",
    "# base64 encode the serialized example\n",
    "b64_encoded_example = base64.b64encode(tfrecord_example).decode(\"utf-8\")\n",
    "\n",
    "# prepare the input example in the correct format\n",
    "input_example = {\"b64\": b64_encoded_example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Just happened a terrible car crash\n",
      "\n",
      "Model output: 0.914532244\n",
      "Prediction: Disaster tweet\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweet:\", input_string)\n",
    "\n",
    "# send prediction request to TF-Serving\n",
    "response = requests.post(serving_url, json={\"instances\": [input_example]})\n",
    "\n",
    "# check the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    model_output = result[\"predictions\"][0][0]\n",
    "    print(\"\\nModel output:\", model_output)\n",
    "\n",
    "    # check if the model output is spam (assuming threshold is 0.75)\n",
    "    tweet_threshold = 0.75\n",
    "    if model_output > tweet_threshold:\n",
    "        print(\"Prediction: Disaster tweet\")\n",
    "    else:\n",
    "        print(\"Prediction: Not Disaster tweet (just normal tweet)\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil output dari prediction request, didapat hasil bahwa contoh tweet yang digunakan untuk pengujian merupakan tweet berjenis Disaster Tweet dengan output model sekitar 0.91. Hasil ini menunjukkan bahwa model bekerja cukup baik dalam mengklasifikasikan kalimat tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster_tweets01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
